# -*- coding: utf-8 -*-
"""function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mDHtoOtRs9_rRHP3PEMS67Kpcwwhkeot
"""

import lxml
import time
import sys
import json
import requests
from textblob import TextBlob
from collections import Counter
import datetime
import nltk
import csv
import re
import csv
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')

YOUTUBE_IN_LINK = 'https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults=100&order=relevance&pageToken={pageToken}&videoId={videoId}&key={key}'
YOUTUBE_LINK = 'https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults=100&order=relevance&videoId={videoId}&key={key}'
key = 'AIzaSyBpy0KaJU4jUqawJRgfC2YpXqOMIdnjJDU'


def commentExtract(videoId, count = -1):
	print ("\nComments downloading")
	page_info = requests.get(YOUTUBE_LINK.format(videoId = videoId, key = key))
	while page_info.status_code != 200:
		if page_info.status_code != 429:
			print ("Comments disabled")
			sys.exit()

		time.sleep(20)
		page_info = requests.get(YOUTUBE_LINK.format(videoId = videoId, key = key))

	page_info = page_info.json()

	comments = []
	co = 0;
	for i in range(len(page_info['items'])):
		#if page_info['items'][i]['snippet']['topLevelComment']['snippet']['likeCount']>=0:
		comments.append(page_info['items'][i]['snippet']['topLevelComment']['snippet']['textOriginal'])
		co += 1
		if co == count:
			return comments

	while 'nextPageToken' in page_info:
		temp = page_info
		page_info = requests.get(YOUTUBE_IN_LINK.format(videoId = videoId, key = key, pageToken = page_info['nextPageToken']))

		while page_info.status_code != 200:
			time.sleep(20)
			page_info = requests.get(YOUTUBE_IN_LINK.format(videoId = videoId, key = key, pageToken = temp['nextPageToken']))
		page_info = page_info.json()

		for i in range(len(page_info['items'])):
			comments.append(page_info['items'][i]['snippet']['topLevelComment']['snippet']['textOriginal'])
			co += 1
			if co == count:
				return comments
	print ()

	return comments

def getComment(videoId_all,text_name):
	count = 1000
	comments = []

	with open('/content/drive/MyDrive/Information code/verified_proxies.json', encoding='utf-8') as f:
		a = json.load(f)

	for videoId in videoId_all:
		requests.adapters.DEFAULT_RETRIES = 20
		s = requests.session()
		flag = 0
		s.keep_alive = False
		s.proxies = {a[flag]['type']:str(a[flag]['host'])+':'+str(a[flag]['port'])}
		flag = flag+1

		comments = comments + commentExtract(videoId, count)
	with open(text_name,'w',encoding='utf-8') as f:
		for i in comments:
			f.write(i+'\n')

#Read file
def read_text(text_name):
# Read positive and negative sentiment words
  positive_words = []
  with open('/content/drive/MyDrive/Information code/positive words.csv', 'r', encoding='utf-8', errors="ignore") as file:
      reader = csv.reader(file)
      for row in reader:
          positive_words.extend(row)

  negative_words = []
  with open('/content/drive/MyDrive/Information code/negative words.csv', 'r', encoding='utf-8', errors="ignore") as file:
      reader = csv.reader(file)
      for row in reader:
          negative_words.extend(row)

  with open(text_name, "r", encoding="utf-8", errors="ignore") as file:
      text = file.read()
  return positive_words,negative_words,text

#NLP

# data cleaning function
#Removes numbers and symbols in the text
def clean(doc):
  expression = r'[^a-zA-Z ]'
  cleantextCAP = re.sub(expression, '', doc)
  cleantext = cleantextCAP.lower()  # lower case
  return cleantext
def nlp(text):
  clean_text = clean(text)
  print(clean_text)
  # decompose a list of sentences into words from NLTK module
  tokens = nltk.word_tokenize(clean_text)
  #Remove stop words
  stop_words = set(stopwords.words('english'))
  # standard syntax
  filtered_sentence = []
  for w in tokens:
      if w not in stop_words:
          filtered_sentence.append(w)

  print(tokens)
  print(filtered_sentence)
  return filtered_sentence

def lemmatize(filtered_sentence):
  lemmatizer = WordNetLemmatizer()
  lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_sentence]
  cleantext_lemmatized = ' '.join(lemmatized_tokens)
  print(cleantext_lemmatized)
  return (cleantext_lemmatized)

# Frequency of occurrence of statistically positive words
def positive_analysis(positive_words,cleantext_lemmatized):
  positive_freq = {}


  for word in positive_words:
      if word in cleantext_lemmatized:
          positive_freq[word] = cleantext_lemmatized.count(word)

  # Sentiment analysis
  positive_count = sum(positive_freq.values())
  total_words = len(cleantext_lemmatized)
  positive_ratio = positive_count / total_words if total_words > 0 else 0
  import pandas as pd
  df = pd.DataFrame(list(positive_freq.items()), columns=['Word', 'Frequency'])
  df = df.sort_values(by='Frequency', ascending=False)
  df = df.reset_index(drop=True)
  print(df.head(10))
  print(f"Total positive words: {positive_count}, Total words: {total_words}, Positive ratio: {positive_ratio:.2%}")
  return positive_ratio
  
  # Statistics on the frequency of occurrence of negative terms
def negative_analysis(negative_words,filtered_sentence):
  negative_freq = {}


  for word in negative_words:
      if word in filtered_sentence:
          negative_freq[word] = filtered_sentence.count(word)

  # Sentiment anaylsis
  negative_count = sum(negative_freq.values())
  total_words = len(filtered_sentence)
  nagetive_ratio = negative_count / total_words if total_words > 0 else 0

  print(f"Total nagetive words: {negative_count}, Total words: {total_words}, Nagetive ratio: {nagetive_ratio:.2%}")
  return (nagetive_ratio)