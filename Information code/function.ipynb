{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Information code')\n",
        "import lxml\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import datetime\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "import csv\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nrF7ibCKBWq",
        "outputId": "4d3944af-4f20-42ce-bb90-2659d792fe0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "YOUTUBE_IN_LINK = 'https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults=100&order=relevance&pageToken={pageToken}&videoId={videoId}&key={key}'\n",
        "YOUTUBE_LINK = 'https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults=100&order=relevance&videoId={videoId}&key={key}'\n",
        "key = 'AIzaSyBpy0KaJU4jUqawJRgfC2YpXqOMIdnjJDU'\n",
        "\n",
        "def commentExtract(videoId):\n",
        "\tcount = 1000\n",
        "\tpage_info = requests.get(YOUTUBE_LINK.format(videoId = videoId, key = key))\n",
        "\twhile page_info.status_code != 200:\n",
        "\t\tif page_info.status_code != 429:\n",
        "\t\t\tprint (\"Comments disabled\")\n",
        "\t\t\tsys.exit()\n",
        "\t\ttime.sleep(20)\n",
        "\t\tpage_info = requests.get(YOUTUBE_LINK.format(videoId = videoId, key = key))\n",
        "\tpage_info = page_info.json()\n",
        "\tcomments = []\n",
        "\tco = 0;\n",
        "\tfor i in range(len(page_info['items'])):\n",
        "\t\t#if page_info['items'][i]['snippet']['topLevelComment']['snippet']['likeCount']>=0:\n",
        "\t\tcomments.append(page_info['items'][i]['snippet']['topLevelComment']['snippet']['textOriginal'])\n",
        "\t\tco += 1\n",
        "\t\tif co == count:\n",
        "\t\t\treturn comments\n",
        "\n",
        "\twhile 'nextPageToken' in page_info:\n",
        "\t\ttemp = page_info\n",
        "\t\tpage_info = requests.get(YOUTUBE_IN_LINK.format(videoId = videoId, key = key, pageToken = page_info['nextPageToken']))\n",
        "\n",
        "\t\twhile page_info.status_code != 200:\n",
        "\t\t\ttime.sleep(20)\n",
        "\t\t\tpage_info = requests.get(YOUTUBE_IN_LINK.format(videoId = videoId, key = key, pageToken = temp['nextPageToken']))\n",
        "\t\tpage_info = page_info.json()\n",
        "\n",
        "\t\tfor i in range(len(page_info['items'])):\n",
        "\t\t\tcomments.append(page_info['items'][i]['snippet']['topLevelComment']['snippet']['textOriginal'])\n",
        "\t\t\tco += 1\n",
        "\t\t\tif co == count:\n",
        "\t\t\t\tprint (\"Successful download\")\n",
        "\t\t\t\treturn comments\n",
        "\n",
        "\treturn comments\n",
        "\n"
      ],
      "metadata": {
        "id": "j-rv8lLS8P-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mews(date,end_date):\n",
        "  api_key = '9201c0ef086e461695d9b784d9c99fbb'\n",
        "\n",
        "  url = f\"https://newsapi.org/v2/everything?q=garmin&language=en&apiKey={api_key}&from='{date}'&to='{end_date}'\"\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    articles = data['articles']\n",
        "    articles = [article for article in articles if 'watch' in article.get('headline', {}).get('main', '').lower() or\n",
        "                            'watch' in article.get('snippet', '').lower()]\n",
        "    return articles\n",
        "  except:\n",
        "    print (\"Can't get further news\")"
      ],
      "metadata": {
        "id": "lIzsH_2Co4tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getComment(videoId_all,text_name,year):\n",
        "\tcomments = []\n",
        "\n",
        "\twith open('verified_proxies.json', encoding='utf-8') as f:\n",
        "\t\ta = json.load(f)\n",
        "\n",
        "\tfor videoId in videoId_all:\n",
        "\t\trequests.adapters.DEFAULT_RETRIES = 20\n",
        "\t\ts = requests.session()\n",
        "\t\tflag = 0\n",
        "\t\ts.keep_alive = False\n",
        "\t\ts.proxies = {a[flag]['type']:str(a[flag]['host'])+':'+str(a[flag]['port'])}\n",
        "\t\tflag = flag+1\n",
        "\n",
        "\t\tcomments = comments + commentExtract(videoId)\n",
        "\twith open(text_name,'w',encoding='utf-8') as f:\n",
        "\t\tfor i in comments:\n",
        "\t\t\tf.write(i+'\\n')\n",
        "\n",
        "\ti = 1\n",
        "\twhile i < 13:\n",
        "\t\ttry:\n",
        "\t\t\tdate = datetime.datetime(year, i, 1)\n",
        "\t\t\tend_date = datetime.datetime(year, i, 28)\n",
        "\t\t\tdate = date.strftime(\"%Y-%m-%d\")\n",
        "\t\t\tend_date = end_date.strftime(\"%Y-%m-%d\")\n",
        "\t\t\tprint (date)\n",
        "\t\t\tnews = get_mews(date,end_date)\n",
        "\n",
        "\t\t\twith open(\"text_name.txt\", \"a\", encoding=\"utf-8\") as file:\n",
        "\t\t\t\t\tfor article in news:\n",
        "\t\t\t\t\t\tfile.write(f\"Title: {article['title']}\\n\")\n",
        "\t\t\t\t\t\tfile.write(f\"Description: {article['description']}\\n\")\n",
        "\t\t\t\t\t\tfile.write(f\"Content: {article['content']}\\n\")\n",
        "\t\t\t\t\t\tfile.write('\\n')\n",
        "\t\t\tprint(\"Title: {article['title']}\\n\")\n",
        "\n",
        "\t\t\ti = i+1\n",
        "\t\t\ttime.sleep(3)\n",
        "\t\texcept:\n",
        "\t\t\tprint(\"Can't get news\")\n"
      ],
      "metadata": {
        "id": "dbcGIo5O9fK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read file\n",
        "def read_text(text_name):\n",
        "# Read positive and negative sentiment words\n",
        "  positive_words = []\n",
        "  with open('/content/drive/MyDrive/Information code/positive words.csv', 'r', encoding='utf-8', errors=\"ignore\") as file:\n",
        "      reader = csv.reader(file)\n",
        "      for row in reader:\n",
        "          positive_words.extend(row)\n",
        "\n",
        "  negative_words = []\n",
        "  with open('/content/drive/MyDrive/Information code/negative words.csv', 'r', encoding='utf-8', errors=\"ignore\") as file:\n",
        "      reader = csv.reader(file)\n",
        "      for row in reader:\n",
        "          negative_words.extend(row)\n",
        "\n",
        "  with open(text_name, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "      text = file.read()\n",
        "  return positive_words,negative_words,text"
      ],
      "metadata": {
        "id": "miUEIZmQGVjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def part_text(text):\n",
        "  first_twenty_words = text[:100]\n",
        "  result  = (first_twenty_words)\n",
        "  return result"
      ],
      "metadata": {
        "id": "EXUcUsP-38mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NLP\n",
        "\n",
        "# data cleaning function\n",
        "#Removes numbers and symbols in the text\n",
        "def clean(doc):\n",
        "  expression = r'[^a-zA-Z ]'\n",
        "  cleantextCAP = re.sub(expression, '', doc)\n",
        "  cleantext = cleantextCAP.lower()  # lower case\n",
        "  return cleantext\n",
        "def nlp(text):\n",
        "  clean_text = clean(text)\n",
        "  print(part_text(clean_text))\n",
        "  # decompose a list of sentences into words from NLTK module\n",
        "  tokens = nltk.word_tokenize(clean_text)\n",
        "  #Remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  # standard syntax\n",
        "  filtered_sentence = []\n",
        "  for w in tokens:\n",
        "      if w not in stop_words:\n",
        "          filtered_sentence.append(w)\n",
        "\n",
        "  print(' '.join(tokens[:20]))\n",
        "  print(part_text(filtered_sentence))\n",
        "  return filtered_sentence"
      ],
      "metadata": {
        "id": "pUS5k28WHf0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(filtered_sentence):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_sentence]\n",
        "  cleantext_lemmatized = ' '.join(lemmatized_tokens)\n",
        "  print(part_text(cleantext_lemmatized))\n",
        "  return (cleantext_lemmatized)\n"
      ],
      "metadata": {
        "id": "FUoucXXaHjWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency of occurrence of statistically positive words\n",
        "def positive_analysis(positive_words,cleantext_lemmatized):\n",
        "  positive_freq = {}\n",
        "\n",
        "\n",
        "  for word in positive_words:\n",
        "      if word in cleantext_lemmatized:\n",
        "          positive_freq[word] = cleantext_lemmatized.count(word)\n",
        "\n",
        "  # Sentiment analysis\n",
        "  positive_count = sum(positive_freq.values())\n",
        "  total_words = len(cleantext_lemmatized)\n",
        "  positive_ratio = positive_count / total_words if total_words > 0 else 0\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(list(positive_freq.items()), columns=['Word', 'Frequency'])\n",
        "  df = df.sort_values(by='Frequency', ascending=False)\n",
        "  df = df.reset_index(drop=True)\n",
        "  print(f\"Total positive words: {positive_count}, Total words: {total_words}, Positive ratio: {positive_ratio:.2%}\")\n",
        "  return positive_ratio"
      ],
      "metadata": {
        "id": "D6ZfqeSHHk1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics on the frequency of occurrence of negative terms\n",
        "def negative_analysis(negative_words,filtered_sentence):\n",
        "  negative_freq = {}\n",
        "\n",
        "\n",
        "  for word in negative_words:\n",
        "      if word in filtered_sentence:\n",
        "          negative_freq[word] = filtered_sentence.count(word)\n",
        "\n",
        "  # Sentiment anaylsis\n",
        "  negative_count = sum(negative_freq.values())\n",
        "  total_words = len(filtered_sentence)\n",
        "  nagetive_ratio = negative_count / total_words if total_words > 0 else 0\n",
        "  print(f\"Total nagetive words: {negative_count}, Total words: {total_words}, Nagetive ratio: {nagetive_ratio:.2%}\")\n",
        "  return (nagetive_ratio)"
      ],
      "metadata": {
        "id": "7uIOSVBwR81f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positive(positive_words,cleantext_lemmatized):\n",
        "  positive_freq = {}\n",
        "\n",
        "\n",
        "  for word in positive_words:\n",
        "      if word in cleantext_lemmatized:\n",
        "          positive_freq[word] = cleantext_lemmatized.count(word)\n",
        "\n",
        "  positive_count = sum(positive_freq.values())\n",
        "  total_words = len(cleantext_lemmatized)\n",
        "  positive_ratio = positive_count / total_words if total_words > 0 else 0\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(list(positive_freq.items()), columns=['Word', 'Frequency'])\n",
        "  df = df.sort_values(by='Frequency', ascending=False)\n",
        "  df = df.reset_index(drop=True)\n",
        "  print(df.head(10))\n",
        "  print(f\"Total positive words: {positive_count}, Total words: {total_words}, Positive ratio: {positive_ratio:.2%}\")\n",
        "  return positive_freq"
      ],
      "metadata": {
        "id": "SzaP_Loh1gra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}