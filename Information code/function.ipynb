{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import lxml\n","import time\n","import sys\n","import json\n","import requests\n","from textblob import TextBlob\n","from collections import Counter\n","import datetime\n","import nltk\n","import csv\n","import re\n","import csv\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nrF7ibCKBWq","executionInfo":{"status":"ok","timestamp":1714332850148,"user_tz":-60,"elapsed":418,"user":{"displayName":"Christina Huang","userId":"06283121662674092922"}},"outputId":"53d1f6c9-a019-49a0-a050-a5e40eb9b75b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\n","YOUTUBE_IN_LINK = 'https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults=100&order=relevance&pageToken={pageToken}&videoId={videoId}&key={key}'\n","YOUTUBE_LINK = 'https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults=100&order=relevance&videoId={videoId}&key={key}'\n","key = 'AIzaSyBpy0KaJU4jUqawJRgfC2YpXqOMIdnjJDU'\n","\n","\n","def commentExtract(videoId, count = -1):\n","\tprint (\"\\nComments downloading\")\n","\tpage_info = requests.get(YOUTUBE_LINK.format(videoId = videoId, key = key))\n","\twhile page_info.status_code != 200:\n","\t\tif page_info.status_code != 429:\n","\t\t\tprint (\"Comments disabled\")\n","\t\t\tsys.exit()\n","\n","\t\ttime.sleep(20)\n","\t\tpage_info = requests.get(YOUTUBE_LINK.format(videoId = videoId, key = key))\n","\n","\tpage_info = page_info.json()\n","\n","\tcomments = []\n","\tco = 0;\n","\tfor i in range(len(page_info['items'])):\n","\t\t#if page_info['items'][i]['snippet']['topLevelComment']['snippet']['likeCount']>=0:\n","\t\tcomments.append(page_info['items'][i]['snippet']['topLevelComment']['snippet']['textOriginal'])\n","\t\tco += 1\n","\t\tif co == count:\n","\t\t\treturn comments\n","\n","\twhile 'nextPageToken' in page_info:\n","\t\ttemp = page_info\n","\t\tpage_info = requests.get(YOUTUBE_IN_LINK.format(videoId = videoId, key = key, pageToken = page_info['nextPageToken']))\n","\n","\t\twhile page_info.status_code != 200:\n","\t\t\ttime.sleep(20)\n","\t\t\tpage_info = requests.get(YOUTUBE_IN_LINK.format(videoId = videoId, key = key, pageToken = temp['nextPageToken']))\n","\t\tpage_info = page_info.json()\n","\n","\t\tfor i in range(len(page_info['items'])):\n","\t\t\tcomments.append(page_info['items'][i]['snippet']['topLevelComment']['snippet']['textOriginal'])\n","\t\t\tco += 1\n","\t\t\tif co == count:\n","\t\t\t\treturn comments\n","\tprint ()\n","\n","\treturn comments\n","\n"],"metadata":{"id":"j-rv8lLS8P-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def getComment(videoId_all,text_name):\n","\tcount = 1000\n","\tcomments = []\n","\n","\twith open('/content/drive/MyDrive/Information code/verified_proxies.json', encoding='utf-8') as f:\n","\t\ta = json.load(f)\n","\n","\tfor videoId in videoId_all:\n","\t\trequests.adapters.DEFAULT_RETRIES = 20\n","\t\ts = requests.session()\n","\t\tflag = 0\n","\t\ts.keep_alive = False\n","\t\ts.proxies = {a[flag]['type']:str(a[flag]['host'])+':'+str(a[flag]['port'])}\n","\t\tflag = flag+1\n","\n","\t\tcomments = comments + commentExtract(videoId, count)\n","\twith open(text_name,'w',encoding='utf-8') as f:\n","\t\tfor i in comments:\n","\t\t\tf.write(i+'\\n')"],"metadata":{"id":"dbcGIo5O9fK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Read file\n","def read_text(text_name):\n","# Read positive and negative sentiment words\n","  positive_words = []\n","  with open('/content/drive/MyDrive/Information code/positive words.csv', 'r', encoding='utf-8', errors=\"ignore\") as file:\n","      reader = csv.reader(file)\n","      for row in reader:\n","          positive_words.extend(row)\n","\n","  negative_words = []\n","  with open('/content/drive/MyDrive/Information code/negative words.csv', 'r', encoding='utf-8', errors=\"ignore\") as file:\n","      reader = csv.reader(file)\n","      for row in reader:\n","          negative_words.extend(row)\n","\n","  with open(text_name, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n","      text = file.read()\n","  return positive_words,negative_words,text"],"metadata":{"id":"miUEIZmQGVjx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#NLP\n","\n","# data cleaning function\n","#Removes numbers and symbols in the text\n","def clean(doc):\n","  expression = r'[^a-zA-Z ]'\n","  cleantextCAP = re.sub(expression, '', doc)\n","  cleantext = cleantextCAP.lower()  # lower case\n","  return cleantext\n","def nlp(text):\n","  clean_text = clean(text)\n","  print(clean_text)\n","  # decompose a list of sentences into words from NLTK module\n","  tokens = nltk.word_tokenize(clean_text)\n","  #Remove stop words\n","  stop_words = set(stopwords.words('english'))\n","  # standard syntax\n","  filtered_sentence = []\n","  for w in tokens:\n","      if w not in stop_words:\n","          filtered_sentence.append(w)\n","\n","  print(tokens)\n","  print(filtered_sentence)\n","  return filtered_sentence"],"metadata":{"id":"pUS5k28WHf0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lemmatize(filtered_sentence):\n","  lemmatizer = WordNetLemmatizer()\n","  lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_sentence]\n","  cleantext_lemmatized = ' '.join(lemmatized_tokens)\n","  print(cleantext_lemmatized)\n","  return (cleantext_lemmatized)\n"],"metadata":{"id":"FUoucXXaHjWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Frequency of occurrence of statistically positive words\n","def positive_analysis(positive_words,cleantext_lemmatized):\n","  positive_freq = {}\n","\n","\n","  for word in positive_words:\n","      if word in cleantext_lemmatized:\n","          positive_freq[word] = cleantext_lemmatized.count(word)\n","\n","  # Sentiment analysis\n","  positive_count = sum(positive_freq.values())\n","  total_words = len(cleantext_lemmatized)\n","  positive_ratio = positive_count / total_words if total_words > 0 else 0\n","  import pandas as pd\n","  df = pd.DataFrame(list(positive_freq.items()), columns=['Word', 'Frequency'])\n","  df = df.sort_values(by='Frequency', ascending=False)\n","  df = df.reset_index(drop=True)\n","  print(df.head(10))\n","  print(f\"Total positive words: {positive_count}, Total words: {total_words}, Positive ratio: {positive_ratio:.2%}\")\n","  return positive_ratio"],"metadata":{"id":"D6ZfqeSHHk1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Statistics on the frequency of occurrence of negative terms\n","def negative_analysis(negative_words,filtered_sentence):\n","  negative_freq = {}\n","\n","\n","  for word in negative_words:\n","      if word in filtered_sentence:\n","          negative_freq[word] = filtered_sentence.count(word)\n","\n","  # Sentiment anaylsis\n","  negative_count = sum(negative_freq.values())\n","  total_words = len(filtered_sentence)\n","  nagetive_ratio = negative_count / total_words if total_words > 0 else 0\n","\n","  print(\"Negative words frequency:\", negative_freq)\n","  print(f\"Total positive words: {negative_count}, Total words: {total_words}, Positive ratio: {nagetive_ratio:.2%}\")\n","  return (nagetive_ratio)"],"metadata":{"id":"7uIOSVBwR81f"},"execution_count":null,"outputs":[]}]}